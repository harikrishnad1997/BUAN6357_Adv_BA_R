---
title: "exam_1_classification"
output: html_document
date: "`r Sys.Date()`"
---

BUAN 6357 2023 Spring (Johnston)

Exam 1: section 2 - classification

A run log of this code is provided as a PDF file.
You may run this code, explore its actions, and add comments as you wish.

Based on class discussions and homework assignments,
You should extend this code as needed in preparation for answering questions about the process presented here.

```{r}

require(data.table)
require(partykit)

```

Adding flags for the code

```{r}

classif    <- c(1, 2, 3)
byCols     <- 2
byRows     <- 1
demo <- T

if(demo) {setwd("/Users/harikrishnadev/Library/Mobile Documents/com~apple~CloudDocs/School Work/Sem 2/BUAN 6357/BUAN6357/exam_materials/exam_1")}

```

Reading training data

```{r}

in1        <- fread(file="classif.dat")
if(demo) {str(in1)}

```

Creating a function to return the fitted values for the glm function (logistic regression)

```{r}

fitLogit   <- function(df,i) {
              df$y            <- 0
              df$y[df$grp==i] <- 1
              t               <- glm(y~.-grp,family=binomial(),data=df)
              return(t$fitted.values)
              }

```

Creating a probabilty function to calculate bayers risk


```{r}

probabilty_function <- function(tbl) {
(d           <- diag(tbl) )

rSums        <- apply(tbl, byRows, sum) # classification outcomes
cSums        <- apply(tbl, byCols, sum) # actual underlying values

(rPC          <- d/rSums )                     # classification "pc" by digit
(cPC          <- d/cSums )                     # classification "pc" by actual underlying

(rBR          <- 1-rPC )                       # Bayes Risk by digit
(cBR          <- 1-cPC )   
pc.correct <- sum(diag(tbl))/sum(tbl)
results_BR <- list(assigned.bayes.risk=cBR,actual.bayes.risk = rBR,pc.correct = pc.correct)
return(results_BR)
}                    # Bayes Risk by actual underlying
```

Getting the probabilty distribution

```{r}

t         <- data.table(idx=1:3, i=1:3) 
tLogit     <- t[,.(fitted=fitLogit(in1,i)), by=.(idx)]
mLogit    <- matrix(tLogit$fitted, ncol=length(classif),byrow=F) 
idxLogit  <- apply(mLogit,byRows,which.max) 
classLogit<- classif[idxLogit] 
rMargin   <- mLogit[,1]+mLogit[,2]+mLogit[,3] 
t1         <- apply(mLogit,byRows,max)
pLogit    <- t1/rMargin 
brLogit   <- 1-pLogit 
(tbl1      <- table(in1$grp,classLogit,dnn=c("grp","class")) )

```
```{r}

probabilty_function(tbl1)

```

Same exercise for Tree classification

```{r}

nmDat      <- in1
nmDat$fac  <- as.factor(nmDat$grp)
mn        <- ctree(fac~.-grp,data=nmDat) 
mnTree     <- as.matrix(predict(mn,type="prob")) 
attr(mnTree,"dimnames") <- NULL
idxTr      <- apply(mnTree,byRows,which.max)
classTree <- classif[idxTr] 
pTree     <- apply(mnTree,byRows,max) 
brTree    <- 1-pTree 
(tbl2      <- table(in1$grp,classTree,dnn=c("grp","class")) )

```

```{r}

probabilty_function(tbl2)

```


